# 综合案例

## 新零售综合案例

数据结构介绍

```properties
InvoiceNo  string  订单编号(退货订单以C 开头)
StockCode  string  产品代码
Description string  产品描述
Quantity integer  购买数量(负数表示退货)
InvoiceDate string   订单日期和时间   12/1/2010 8:26
UnitPrice  double  商品单价
CustomerID  integer  客户编号
Country string  国家名字

字段与字段之间的分隔符号为 逗号
```

相关的需求(DSL和SQL):

```properties
(1) 客户数最多的10个国家
(2) 销量最高的10个国家
(3) 各个国家的总销售额分布情况
(4) 销量最高的10个商品
(5) 商品描述的热门关键词Top300
(6) 退货订单数最多的10个国家
(7) 月销售额随时间的变化趋势
(8) 日销量随时间的变化趋势
(9) 各国的购买订单量和退货订单量的关系
(10) 商品的平均单价与销量的关系
```

### 完成数据清洗过滤的操作

清洗需求:

```properties
清洗需求:
	需求一: 将客户id(CustomerID) 为 0的数据过滤掉 
	需求二: 将商品描述(Description) 为空的数据过滤掉
	需求三: 将日期格式进行转换处理:
		原有数据信息: 12/1/2010 8:26
		转换为: 2010-12-01 08:26

将清洗的结果写出到HDFS上 /xls/output
```

代码实现:

```properties
from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession
import pyspark.sql.functions as F
import os

# 锁定远端python版本:
os.environ['SPARK_HOME'] = '/export/server/spark'
os.environ['PYSPARK_PYTHON'] = '/root/anaconda3/bin/python3'
os.environ['PYSPARK_DRIVER_PYTHON'] = '/root/anaconda3/bin/python3'

if __name__ == '__main__':
    print("完成新零售的相关需求")

    # 1- 创建SparkSession对象
    spark = SparkSession.builder.master('local[*]').appName('xls_clear').getOrCreate()

    # 2- 读取外部文件数据
    df_init = spark.read.csv(
        path='file:///export/data/workspace/sz30_pyspark_parent/_04_xls_project/data/E_Commerce_Data.csv',
        header=True,
        inferSchema=True
    )

    # 3- 对数据执行清洗操作

    df_clear = df_init.where('CustomerID != 0 and Description is not null')
    df_clear = df_clear.withColumn(
        'InvoiceDate',
        F.from_unixtime(F.unix_timestamp(df_clear['InvoiceDate'],'M/d/yyyy H:mm'),'yyyy-MM-dd HH:mm')
    )

    # 4- 需要将清洗转换后的结果写出到HDFS上: /xls/output
    df_clear.write.mode('overwrite').csv(
        path='hdfs://node1:8020/xls/output',
        sep='\001'
    )

    # 5- 关闭spark对象
    spark.stop()
```

### 需求统计分析操作

```python

```

